# 服务化参数校验

## 1 简介

服务化参数校验提供服务化场景下参数比对的能力，包含：

* 模型初始化文件的比对(config.json和generation_config.json);
* 用户从前端传入服务化框架前的参数比对，支持比对的参数列表如下所示;
* 从服务化框架传入模型前的参数比对，支持比对的参数列表如下所示;
   | parameters              | 
   |-------------------------|
   | temperature             |           
   | top_k                   |
   | top_p                   |
   | do_sample               |
   | seed                    |
   | repetition_penalty      |
   | watermark               |
   | frequency_penalty       |
   | presence_penalty        |
   | length_penalty          |
   | ignore_eos              |


   
比对结束后会打印比对结果报告，罗列比对结果不同的服务化参数，并落盘为CSV文件。如果GPU和NPU、NPU和NPU结果完全相同则打印No differences found。

## 2 使用方式

### 前置准备
1. 请参见链接[安装msit](../install/README.md)。

2. 准备比对脚本params_check.py。
    ```
    import argparse
    from msit_llm.parameters_check.service_parameters_check import service_params_check

    def main():
        parser = argparse.ArgumentParser(description="Compare two files and generate a report.")
        parser.add_argument("--input1", dest='input1', default='', type=str,
                            help="<Optional> Path of GPU/NPU file", required=True)
        parser.add_argument("--input2", dest='input2', default='', type=str,
                            help="<Optional> Path of NPU file", required=True)
        args = parser.parse_args()

        service_params_check(args.input1, args.input2)

    if __name__ == "__main__":
        main()
    ```
    其中--input1表示NPU/GPU的输入文件，--input2表示NPU的输入文件。


### 2.1 模型初始化文件的比对
1. 下载模型权重文件夹，可在[huggingFace](https://huggingface.co/)，[gitee](https://gitee.com/hf-models), [魔搭社区](https://www.modelscope.cn/home)上下载。
   
   文件夹下的结构通常包含：
    ```
    model_name  
    ├-- LICENSE
    ├-- README.md   
    ├-- config.json
    ├-- configuration.json 
    ├-- generation_config.json
    ├-- model.safetensors.index.json
    ├-- special_tokens_map.json
    ├-- tokenizer.json
    ├-- tokenizer_config.json
    ├-- model-xxxxx-of-xxxxx.safetensors   
    └-- ...                                 
    ```
2. 输入GPU/NPU和NPU模型的初始化文件(config.json和generation_config.json)进行比对，运行命令如下：
    ```
    python params_check.py --input1 ./qwen2-7b/gpu/config.json --input2 ./qwen2-7b/npu/config.json
    python params_check.py --input1 ./qwen2-7b/gpu/generation_config.json --input2 ./qwen2-7b/npu/generation_config.json
    ```
    最后会输出比对结果打屏日志，罗列GPU和NPU或者NPU和NPU模型初始化文件不同的参数和值，并落盘为CSV文件，格式如下所示。如果GPU和NPU或者NPU和NPU的两边的初始化文件完全相同，则输出No differences found。
    
   | param                   | file1_value | file2_value |
   |-------------------------|-------------|-------------|
   | max_position_embeddings | 32768       | 131072      |
   | ...                     | ...         | ...         |

    其中param代表比较的参数名，file1_value和file2_value分别表示文件1中相应参数的值和文件2中相应参数的值。

    如果param在input1中存在，但是在input2中不存在，则file2_value的值为**N/A**。
   
### 2.2 从前端传入服务化框架前的参数比对
1. GPU侧安装VLLM：
   ```
   pip install vllm
   ```
   查看vllm安装路径：
   ```
   pip show vllm
   ```
   在vllm中llm_engine.py的add_request函数中获取服务化参数，并落盘为txt文件，如：

   a. vim /usr/local/miniconda3/envs/xxx/lib/python3.10/site-packages/vllm/engine/llm_engine.py

   b. 在add_request函数中插入如下代码：
   ```
   with open(保存为txt的路径, "a") as f:
       f.write(f"{request_id}:{str(params)}")
       f.write("\n")
   ```
   运行服务化命令并发送请求即可获得GPU侧用户从前端送入服务化框架前的参数。


2. NPU侧安装**最新的**[mindie-service](https://gitcode.com/Ascend/MindIE-Turbo)

   开启debug日志：
   ```
   在环境变量中添加
   export MINDIE_LOG_LEVEL="server:debug"
   ```
   运行服务化命令并发送请求，落盘的mindie-server.log中包含NPU侧用户从前端送入服务化框架前的参数。


3. 比对GPU的txt文件和NPU的mindie-server.log，得到两者参数比较结果，命令如下：
    ```
   python params_check.py --input1 ./gpu.txt --input2 ./mindie-server.log
    ```
   比较的参数包含：temperature, top_k, top_p, do_sample, seed, repetition_penalty, watermark, frequency_penalty, presence_penalty, length_penalty, ignore_eos。
   
   最后会输出比对结果打屏日志，罗列GPU和NPU或者NPU和NPU不同的服务化参数和值，并落盘为CSV文件，格式如下所示。如果GPU和NPU或者NPU和NPU的两边的服务化参数完全相同，则输出No differences found。
   | req_order | param | file1_value | file2_value |
   |-----------|-------|-------------|-------------|
   | 1         | top_k | -1          | 0.95        |
   | 2         | top_p | 0.95        | 0.90        |
   | ...       | ...   | ...         | ...         |


   其中req_order表示发送的请求数量（**两边发送请求的顺序需保持一致**），param代表比较的参数名，file1_value和file2_value分别表示文件1中相应参数的值和文件2中相应参数的值。
   如果param在input1中存在，但是在input2中不存在，则file2_value的值为**N/A**。   

4. 同时支持比对NPU和NPU的mindie-server.log，得到两者参数比较结果，命令如下：
    ```
    python params_check.py --input1 ./mindie-server1.log --input2 ./mindie-server2.log
    ```
### 2.3 从服务化框架传入模型前的参数比对
1. GPU侧安装VLLM：
   ```
   pip install vllm
   ```
   查看vllm安装路径：
   ```
   pip show vllm
   ```
   在vllm中llm_engine.py的add_request函数中获取服务化参数，并落盘为txt文件，如：
   
   a. vim /usr/local/miniconda3/envs/xxx/lib/python3.10/site-packages/vllm/engine/llm_engine.py

   b. 在step函数中保存execute_model_req中的服务化参数，在ExecuteModelRequest后插入如下代码：
   ```
   with open(保存为txt的路径, "a") as f:
        f.write(f"{execute_model_req.seq_group_metadata_list[0].request_id}: {str(execute_model_req.seq_group_metadata_list[0].sampling_params)}")
        f.write(f"do_sample:{execute_model_req.seq_group_metadata_list[0].do_sample}")
        f.write("\n")
   ```
   运行服务化命令并发送请求即可获得GPU侧从服务化框架传入模型前的参数。


2. NPU侧安装**最新的**[mindie-service](https://gitcode.com/Ascend/MindIE-Turbo)

   开启debug日志：
   ```
   在环境变量中添加
   export MINDIE_LOG_LEVEL=debug
   ```
   运行服务化命令并发送请求，在~/mindie/log/debug落盘的mindie-llm.log中包含NPU侧从服务化框架传入模型前的参数。


3. 比对GPU的txt文件和NPU的mindie-llm.log，得到两者参数比较结果，命令如下：
    ```
   python params_check.py --input1 ./gpu.txt --input2 ./mindie-llm.log
    ```
   比较的参数包含：temperature, top_k, top_p, do_sample, seed, repetition_penalty, watermark, frequency_penalty, presence_penalty, length_penalty, ignore_eos。
   
   最后会输出比对结果打屏日志，罗列GPU和NPU或者NPU和NPU不同的服务化参数和值，并落盘为CSV文件，格式如下所示。如果GPU和NPU或者NPU和NPU的两边的服务化参数完全相同，则输出No differences found。
   | req_order | param | file1_value | file2_value |
   |-----------|-------|-------------|-------------|
   | 1         | top_k | -1          | 0.95        |
   | 2         | top_p | 0.95        | 0.90        |
   | ...       | ...   | ...         | ...         |


   其中req_order表示发送的请求数量（**两边发送请求的顺序需保持一致**），param代表比较的参数名，file1_value和file2_value分别表示文件1中相应参数的值和文件2中相应参数的值。
   如果param在input1中存在，但是在input2中不存在，则file2_value的值为**N/A**。   

4. 同时支持比对NPU和NPU的mindie-llm.log，得到两者参数比较结果，命令如下：
    ```
    python params_check.py --input1 ./mindie-llm1.log --input2 ./mindie-llm2.log
    ```