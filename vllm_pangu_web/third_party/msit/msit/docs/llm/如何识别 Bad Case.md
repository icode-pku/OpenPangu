# 发现 bad case
一般来说，有两种使用最多的方案：

1. 单精度测试（single precision）
2. 数据集测试（dataset）

如果这两种方案均无法发现 `bad case`，则可以认为模型不存在精度问题。

需要注意的是，在昇腾大模型推理场景中，可能存在算子未正常接入的情况。推理过程不会中断，报错信息会被记录在日志里，但是模型的输出可能存在乱码的情况。因此，使用昇腾大模型进行推理时，要先确认模型本身是否异常。模型本身无误还存在输出错误，那么就确定为精度问题了。

## 单精度测试

在 [前置知识](./大模型精度问题定位全流程.md) 中，我们发现测试模型存在问题，是源自于一个输入 $x=2$。我们发现在这个输入下，两个模型的结果出现不一致。由于我们无条件相信标杆模型，所有我们认为测试模型的结果存在问题。这个就叫做 **单精度测试**。

在大模型场景中也是如此，即使存在些许差异。我们同样人为的准备一个 $x$，这个输入又被称为提示词（prompt），请求（query）。然后，让大模型根据这个输入进行回答，再比较两个模型的结果。比如在量化场景下，用的最多的提示词是：`中国的首都是哪里？`，或者 `华为的董事长是谁？` 等等。

这里需要注意的是，在 [前置知识](./大模型精度问题定位全流程.md) 中，模型的输出结果是存在 **标准答案（ground truth）** 的。但是大模型的回答是一句话，如果给模型开放性的题目，是不存在标准答案的。因此，我们单精度测试，只是对模型的回答进行一个初步筛查，判断其是否存在胡言乱语的情况。

## 数据集测试

如果单精度测试发现不了问题，可以通过数据集测试进一步确认。

在 [前置知识](./大模型精度问题定位全流程.md) 中，我们准备了 `1000` 个问题打算进行进一步测试，这个就是数据集。在实际业务场景中，常用的数据集有 `BoolQ` 和 `HumanEval`，每个数据集针对不同的场景：

- `BoolQ`：判断题
- `HumanEval`：代码题

使用数据集测试来识别 `bad case` 的流程较为繁琐。首先，让测试模型和标杆模型同时跑一遍数据集，按照模型的输出和数据集内提供的标准答案进行比较，分别统计两个模型的平均分数。如果测试模型平均分远远低于标杆模型平均分，那么我们认为测试模型 **掉点严重**，存在精度问题。如果分数相差无几，那么我们认为模型不存在精度问题，不需要进一步定位。那么何为相差无几呢？实际上，不同的模型，在不同的业务场景下有各自的验收标准，根据验收标准来决定多大的掉点算作精度问题。

如果模型掉点严重，我们下一步就是找到数据集中的 `bad case`。但是这里识别 `bad case` 的方法和单精度测试下略有不同。在数据集测试场景下，我们需要考虑标杆模型的答案，测试模型的答案和数据集标准答案之间的关系。一般的，我们只会考虑两个场景，如下：

### 他对我不对

所谓 *他对我不对*，意思就是标杆模型的输出和数据集标准答案一致，但是测试模型不一致。由于我们希望测试模型的表现尽量靠近标杆模型，所以我们要求标杆对的我也对。因此，凡是标杆模型答对的，但是测试模型没有答对的题目，都属于 `bad case`。

### 他对我也对

这种情况基本不会被考虑，因为不符合控制变量法范畴，但是在其他方法都无效的情况下，可以试试这个方法。

这种情况下，测试模型的输出和标杆模型的输出都和数据集标准答案一致，按理说我们不需要考虑这种场景。但是在某些极端情况下，这类数据能够帮助我们进一步定位精度问题。

在大模型推理场景下，我们的中间结果往往不是整数，而是小数。如果两个算子的 `out tensor`，在一些误差评测指标下，处于一种不好不坏的情况，我们应该认为它存在精度问题还是不存在精度问题呢？比如，两个算子的 `out tensor`，余弦相似度给出 `0.9888`，其实已经很高了。但是我们发现其他的算子都是 `0.99` 左右，那么这个算子到底是否存在精度问题呢？

此时，我们可以尝试考虑两个模型输出都对的情况。由于两个模型表现一致，则我们可以默认算子的表现也一致。那么算子表现一致的情况下，我们可以收集到正常情况下，每个算子在不同误差评测指标下的范围是什么样的。比如上述的例子中，如果我们发现两个模型表现一致的情况下，这个算子出现了 `0.9807` 的情况，说明这个算子在 `0.9807` 的时候就可以达到标准，那么 `0.9888` 肯定也是可以的，所以不存在精度问题。但是这种横向比较，是在逻辑上是不符合控制变量法范畴的，因为模型的输入不一样。但是绝望时的自救还是可以的。

因此，这种情况下的数据，并不是 `bad case`，但是有助于我们处理 `bad case`。一般情况不考虑这场景。