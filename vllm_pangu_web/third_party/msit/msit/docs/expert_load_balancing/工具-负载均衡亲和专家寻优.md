# 负载均衡工具

## 简介

负载均衡工具（Expert Load Balancing Tools)）是在MoE模型静态/动态场景下进行负载均衡亲和专家寻优。通过加载模型推理时dump到的专家热度信息文件（格式为csv或pt），经过算法计算后落盘prefill或decoder阶段的配置文件。

目前提供两种场景下的算法：
- 静态场景 + Atlas 800I A2推理服务器：计算通信负载均衡算法（C2LB）、speculative-moe level 1算法、speculative-moe level 2算法、speculative-moe level 1混置算法 和 speculative-moe level 2混置算法；
- 静态场景 + Atlas 800I A3推理服务器：speculative-moe level 1和speculative-moe level 2算法；
- 动态场景 + Atlas 800I A2推理服务器：计算通信负载均衡算法（C2LB）生产初始配置文件。    


**使用前准备**

#### 环境准备
1. 准备一台基于昇腾NPU的推理服务器。
2. 安装Python3.9及以上版本。

#### 安装操作
1. 安装msIT工具，软件安装 [负载均衡亲和专家寻优工具安装](../install/README.md)。
2. 安装好msIT后需要安装msIT中的elb组件，执行msit install elb。

#### 约束
使用speculative-moe level 1和speculative-moe level 2算法时，针对prefill和 decode 场景需要分开将对应文件输入工具，输入路径不能同时包含prefill和decode。


## 功能介绍

**命令格式**

 ```Shell
msit elb -icp <info_csv_path> -dt <device_type> [options]
```

**参数说明**


| 参数      | 是否必填                   | 使用说明                                                     | 
| ----------- | ---------------------- | ------------------------------------------------------------ | 
| --info-csv-path, -icp  | 是       | 读取专家热度信息文件的路径。数据类型：str   无默认值，传入路径必须存在。    |
| --output-dir, -o       | 否       | 输出最终配置文件的路径。数据类型：str，默认为当前路径，如传入路径则路径必须存在。  |
| --num-redundant-expert, -nre  | 否       | 冗余专家数量。数据类型：int，冗余专家数量为所有卡上的总数，默认值为64。      |
| --num-share-expert-devices, -nsed  | 否       | 独立部署共享专家的卡数。数据类型：int，共享专家数量为 A3 外置专家数量或 A2 混置专家数量，适用于 A2/A3 场景下speculative-moe 算法，默认值为0。      |
| --num-nodes, -nd       | 否       | 节点数，即为机器数。数据类型：int，默认值为8。            |
| --num-npus, -nn        | 否       | npu卡数。数据类型：int，默认值为64。            |
| --algorithm, -al       | 否       | 选择算法类型。数据类型：str，0代表计算通信负载均衡算法（C2LB）， 1代表speculative-moe level 1算法，2代表动态场景下C2LB算法（生产初始配置文件），3代表speculative-moe level 2算法，4代表speculative-moe level 1混置算法，5代表speculative-moe level 2混置算法，默认选择值为3。 |
| --device-type, -dt  | 是      | 服务器类型，a2 代表适用于Atlas 800I A2服务器部署, a3 代表适用于Atlas 800I A3服务器部署。数据类型：str，无默认值，需要自行输入。            | 
| -h, --help             | 否       | 工具使用帮助信息。打印工具的命令帮助信息。        | 


### 注意事项

本工具所需的负载均衡专家热度信息文件依赖MindIE提供的dump能力，需要使能两个mindie的环境变量来获得输入文件，保证在纯模型或服务化场景下采集业务数据或数据集，能正常完成模型推理的前提下，设置export MINDIE_ENABLE_EXPERT_HOTPOT_GATHER=1，设置export MINDIE_EXPERT_HOTPOT_DUMP_PATH=“用户自定义路径”。同时也支持VLLM-Ascend服务化推理框架的专家热度输入。

## 附录

###  算法介绍

#### C2LB算法

###### 要求冗余专家数量需要小于等于NPU的数量

对于MoE类模型，在使用专家并行（EP）时，不同的专家会被分配到不同的GPU/NPU上，由于不同专家的负载可能会根据当前工作负载而变化，因此保持不同GPU/NPU的负载均衡非常重要。
C2LB（Compute Communication Load Balance）计算通信负载均衡算法是一种静态专家放置策略，基于离线统计的专家负载信息，全局考虑计算通信均衡，输出专家在卡上的放置策略。该算法支持冗余专家放置，支持卡上专家数量不同的部署，当前约束卡的数量大于等于冗余专家数量，且每个卡只能部署一个冗余专家。受限于数据集离线统计的专家负载信息的偏差，静态专家部署方案难以应对负载变化大的场景。


#### speculative moe 算法

MoE推理场景下，不同专家激活量天然冷热不均，使得专家计算和all2all通信负载严重失衡，导致快慢卡和资源空泡，需要通过负载均衡手段提升性能。Speculative-moe算法是一类专家均衡部署优化算法，应用于推理系统初始化阶段。调用该算法生成全局冗余专家部署表，应用后可以显著提升负载均衡度、实现推理性能优化。其核心优势包括：  
1） 全面支持A2/A3代际、Prefill/Decode场景，适配多种专家部署形态（共享专家内置、位置、混置（注））；  
2） 基于专家热度细粒度潮汐特征挖掘策略，协同推理框架侧冗余专家设计、高效热度采集与专家部署形态优化，实现多场景负载均衡度提升，端到端性能提升15%+。
其中，Level 1算法通过多阶段潮汐热度实现冷热专家快速匹配，可在亚分钟时间内完成部署计算，开启方式：al 1（非混置）；al 4（混置）；Level 2算法在Level 1基础上进一步使用随机混合整数线性规划（SMILP）与黑盒优化进行更高精度寻优，负载均衡度可以进一步优化5%+，开启方式：al 3（非混置）；al 5（混置）。

**注**：  
内置：共享/非共享专家在每张卡上共同部署；  
外置：共享/非共享专家在不同卡上分离部署；  
混置：共享/非共享专家灵活组合部署，共享专家择优选择布放位置。

**优选推荐**：以上所有算法中，Speculative-moe Level 2混置算法（al 5）在A2推理服务器中取得最优性能，Speculatvie-moe Level 2算法（al 3）在A3超节点中取得最优性能。
