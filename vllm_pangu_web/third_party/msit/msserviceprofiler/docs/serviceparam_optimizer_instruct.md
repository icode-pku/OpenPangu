# 服务化自动寻优工具
## 简介

**服务化自动寻优工具**（Serviceparam Optimizer）是一个基于PSO粒子寻优算法的服务化参数自动寻优工具，支持对 `MindIE` 和 `VLLM` 进行自动寻优，获取符合时延要求的最佳吞吐参数组合。

工具支持仿真与轻量化两种模式，主要包括三大核心功能模块：
- **参数寻优模块**：利用PSO粒子寻优算法自动生成服务化参数组合，不断逼近最优解；同时，Early Rejection算法通过理论建模、调优经验及部分实测数据对服务化参数完成早期评估；

- **仿真模块**：基于XGBoost模型对大模型推理时长进行精准预测，结合服务化调度的虚拟时间轴技术，加速服务化参数验证速度。

- **参数验证模块**：自动化启动服务化进程与测评工具进程，进行参数测试，获取性能结果。当前已支持的测评工具包括ais_bench，vllm_benchmark。

> [](public_sys-resources/icon-note.gif) **注意：**
> 由于benchmark即将下线并由ais_bench代替，寻优工具当前已取消支持benchmark。

服务化自动寻优工具能够基于以上功能模块，自动推荐吞吐较优的服务化参数组合，使用时有两种模式：

- [轻量化模式](#轻量化模式) 
- [仿真模式](#仿真模式) 

目前工具已基于llama3-8b和qwen3-8b通过验证，理论上不限制支持模型范围，后续计划扩大支持范围的验证。

**基本概念**

- `MindIE`、`VLLM`：服务化框架，支持对模型进行服务化部署。
- `Ais_Bench`、`VLLM_Benchmark`：推理性能评测工具，支持对服务化进行推理性能评测。

## AI处理器支持情况<a name="ZH-CN_TOPIC_0000002479925980"></a>

>![](public_sys-resources/icon-note.gif) **说明：** 
>AI处理器与昇腾产品的对应关系，请参见《[昇腾产品形态说明](https://www.hiascend.com/document/detail/zh/AscendFAQ/ProduTech/productform/hardwaredesc_0001.html)》。

|AI处理器类型|是否支持|
|--|:-:|
|Ascend 910C|x|
|Ascend 910B|√|
|Ascend 310B|x|
|Ascend 310P|√|
|Ascend 910|x|


>![](public_sys-resources/icon-notice.gif) **须知：** 
>针对Ascend 910B，当前仅支持该系列产品中的Atlas 800I A2 推理产品。
>针对Ascend 310P，当前仅支持该系列产品中的Atlas 300I Duo 推理卡+Atlas 800 推理服务器（型号：3000）。


## 使用前准备
**环境准备**

1. 准备好能正常运行服务化（如`MindIE Service/VLLM Server`，参见[服务化部署](https://www.hiascend.com/document/detail/zh/mindie/22RC1/mindieservice/servicedev/mindie_service0004.html)）和测评工具（如`vllm_benchmark/ais_bench`，参见[测评工具部署](https://gitee.com/aisbench/benchmark/blob/master/README.md)）的环境。
2. 安装自动寻优工具，命令如下：
    ```
    git clone https://gitcode.com/Ascend/msit.git	
    cd msit/msserviceprofiler
    pip install -e .[real] 
    ```
    使用轻量化的方式进行寻优则只需安装最少的依赖即可，仿真模式需要额外的依赖。
    ```
    pip install -e .[speed]  
    ```
    如果上述安装失败，可尝试安装较少依赖的第三方包，但训练模型时，大数据量时性能较低。
    ```
    pip install -e .[train] 
    ```
**仿真模式版本配套关系**

| 版本配套关系 |     CANN     |     框架     |
|:-------------:|:------------:|:--------------:|
|     MindIE当前版本      | CANN 8.3.RC2 | MindIE 2.2.RC1 |
|     VLLM当前版本      | CANN 8.2.RC1 | VLLM 0.8.4 |

**约束**

由于工具涉及使用MindIE镜像，需遵从其启动方式，PD分离场景中，MindIE使用k8s等技术，需用户自行注意相关风险。

## 快速入门
1. 完成[使用前准备](#使用前准备)章节要求。

2. 修改配置文件：启动寻优前需用户按照实际情况配置[`config.toml`](../msserviceprofiler/modelevalstate/config.toml)，包括寻优参数、测评工具参数、服务化参数。参考[配置文件说明](#配置文件说明)章节完成配置。

3. 启动寻优：完成上述步骤后，执行以下命令，一键启动轻量化自动寻优：
    ```
    msserviceprofiler optimizer
    ```
    默认执行的是基于`Ais_Bench`的`MindIE`服务化参数寻优。

4. 查看结果：寻优时间由模型大小和数据集大小决定，一般在4~8小时完成，结束后会生成`data_storage_*.csv`的文件并保存在当前目录的`result/store`子目录中，其中记录了各组参数的性能，详细介绍请参见[输出文件说明](#输出文件说明)。
## 轻量化模式
**功能说明**

注重精度和可靠性，结合参数验证、参数寻优模块，通过真机实测给出可靠的服务化参数推荐值。

**注意事项**

无

**命令格式**

```
msserviceprofiler optimizer [options]
```
**参数说明**

|参数|可选/必选|说明|
|---|---|---|
|-lb或--load_breakpoint|可选|控制是否从断点恢复寻优过程，配置本参数表示开启，默认未配置表示关闭。|
|-d或--deploy_policy|可选|设置部署策略，即单机或多机部署，可取值：<br>&#8226;single：单机部署<br>&#8226;multiple：多机部署。<br/>默认值为`single`。|
|--backup|可选|决定是否在寻优过程中备份数据，配置本参数表示开启备份，可取值：<br>&#8226;True：开启备份<br>&#8226;False：不开启备份。<br/>默认值为`False`。|
|-b或--benchmark_policy|可选|指定测评工具，可取值：<br>&#8226;vllm_benchmark：使用vllm_benchmark作为测试工具 <br/>&#8226;ais_bench：使用ais_bench作为测试工具<br/>默认值为`ais_bench`。<br/>用户需自行选择适配的推理框架以及测试框架。|
|-e或--engine|可选|指定推理框架，可取值：<br>&#8226;mindie：使用MindIE作为推理框架<br>&#8226;vllm：使用VLLM作为推理框架<br/>默认值为`mindie`。|
|--pd|可选|指定推理框架模式pd竞争或pd分离，可取值：<br>&#8226;competition：pd竞争模式<br>&#8226;disaggregation：pd分离模式<br/>默认值为`competition`。|

**使用示例（mindie服务化参数寻优）**

1. 修改配置文件：启动寻优前需用户按照实际情况配置[`config.toml`](../msserviceprofiler/modelevalstate/config.toml)，包括寻优参数、测评工具参数、服务化参数。参考[配置文件说明](#配置文件说明)章节完成配置。

2. 如果需要设置环境变量作用于mindie/vllm服务，只需在运行工具前设置环境变量即可，例如：
    ```
    export ASCEND_RT_VISIBLE_DEVICES=0
    ```
    工具会在寻优过程中自动设置（仿真和轻量化模式均适用）。

3. 前置条件准备就绪后，执行以下命令，一键启动轻量化自动寻优：
    ```
    msserviceprofiler optimizer
    ```
**使用示例（vllm服务化参数寻优）**

1. 修改配置文件：启动寻优前需用户按照实际情况配置[`config.toml`](../msserviceprofiler/modelevalstate/config.toml)，包括寻优参数、测评工具参数、服务化参数。参考[配置文件说明](#配置文件说明)章节完成配置。
2. 如果需要设置环境变量作用于mindie/vllm服务，只需在运行工具前设置环境变量即可，例如：
    ```
    export ASCEND_RT_VISIBLE_DEVICES=0
    ```
    工具会在寻优过程中自动设置（仿真和轻量化模式均适用）。

3. 前置条件准备就绪后，执行以下命令，一键启动轻量化自动寻优：
    ```
    msserviceprofiler optimizer -e vllm
    ```
    若在VLLM场景下使用`vllm_benchmark`测评工具可参考
    ```
    msserviceprofiler optimizer -e vllm -b vllm_benchmark
    ```
**输出说明**

自动寻优完成后，输出csv格式的结果文件，在当前目录下生成result/store文件夹存放。详情介绍请参见[输出结果文件说明](#输出结果文件说明)。
## 仿真模式

**功能说明**

仿真模式注重速度及资源占用，调动所有模块快速、精确地预测各组参数的吞吐，在较低NPU资源占用的前提下给出服务化参数推荐值。

**注意事项**

仿真模式需要先基于服务化采集数据进行训练，参照[服务化调优工具手册](https://www.hiascend.com/document/detail/zh/mindstudio/80RC1/T&ITools/Profiling/mindieprofiling_0001.html) 开启profiling实际跑一遍MindIE推理服务的测试脚本，将采集的profiling数据进行解析然后用于训练模型。profiling采集数据需要包括batch_type，batch_size，forward_time，batch_end_time(ms)，request_recv_token_size，request_reply_token_size，need_blocks，request_execution_time(ms)，first_token_latency(ms)。

**命令格式**
- train
    ```
    msserviceprofiler train [options]
    ```
- optimizer
    ```
    msserviceprofiler optimizer [options]
    ```
**train训练模型参数说明**

|参数|可选/必选|说明|
|---|---|---|
|-i或--input|必选|输入数据目录，这里所需的数据即profiling的输出路径。|
|-o或--output|可选|输出目录，建议输出在modelevalstate下面创建一个/result/latency_model目录来存放。若未指定则会在当前目录下生成。|
|-t或--type|可选|框架类型，可选值：<br>&#8226;mindie：使用MindIE作为推理框架<br>&#8226;vllm：使用VLLM作为推理框架<br/>默认值为`mindie`。|

**optimizer寻优参数说明**

|参数|可选/必选|说明|
|---|---|---|
|-lb或--load_breakpoint|可选|控制是否从断点恢复寻优过程，配置本参数表示开启，默认未配置表示关闭。|
|-d或--deploy_policy|可选|设置部署策略，即单机或多机部署，可取值：<br>&#8226;single：单机部署<br>&#8226;multiple：多机部署。<br/>默认值为`single`。|
|--backup|可选|决定是否在寻优过程中备份数据，配置本参数表示开启备份，可取值：<br>&#8226;True：开启备份<br>&#8226;False：不开启备份。<br/>默认值为`False`。|
|-b或--benchmark_policy|可选|指定测评工具，可取值：<br>&#8226;vllm_benchmark：使用vllm_benchmark作为测试工具 <br/>&#8226;ais_bench：使用ais_bench作为测试工具<br/>默认值为`ais_bench`。<br/>用户需自行选择适配的推理框架以及测试框架。|
|-e或--engine|可选|指定推理框架，可取值：<br>&#8226;mindie：使用MindIE作为推理框架<br>&#8226;vllm：使用VLLM作为推理框架<br/>默认值为`mindie`。|
|--pd|可选|指定推理框架模式pd竞争或pd分离，可取值：<br>&#8226;competition：pd竞争模式<br>&#8226;disaggregation：pd分离模式<br/>默认值为`competition`。|

**使用示例**

1. 修改配置文件：启动寻优前需用户按照实际情况配置[`config.toml`](../msserviceprofiler/modelevalstate/config.toml)，包括寻优参数、测评工具参数、服务化参数。参考[配置文件说明](#配置文件说明)章节完成配置。

2. 训练模型
    ```
    msserviceprofiler train -i=/path/to/input -o=/path/to/output 
    ```
3. 寻优时需开启环境变量
    ```
    export MODEL_EVAL_STATE_ALL=True
    export MODEL_EVAL_STATE_IS_SLEEP_FLAG=True
    export PYTHONPATH=msit/msserviceprofiler/:$PYTHONPATH #需根据实际路径修改
    ```
4. 启动仿真模式寻优
    ```
    msserviceprofiler optimizer -e vllm -b vllm_benchmark
    ```
**输出说明**

自动寻优完成后，输出csv格式的结果文件，在当前目录下生成result/store文件夹存放。详情介绍请参见[输出结果文件说明](#输出结果文件说明)。


## 输出结果文件说明
输出csv中的每一行对应一组参数，前四列为性能指标。用户可以根据需求筛选满足要求的性能行，将MindIE参数以及ais_bench/vllm_benchmark的参数改为csv中的数据即可。
| 字段 | 说明 |
| --- | --- |
| generate_speed | 吞吐。 |
| time_to_first_token | ttft时延，单位为秒。 |
| time_per_output_token | tpot时延，单位为秒。 |
| success_rate | 测试返回请求成功率。 |
| throughput | 测试吞吐，单位为请求数/秒。 |
| CONCURRENCY | 并发数。 |
| REQUESTRATE | 发送速率。 |
| error | 记录这次参数没有正常执行的原因，在发送错误时记录。 |
| backup | 数据记录地址，当开启--backup时记录。 |
| real_evaluation | 标记数据是否由真实测试结果得到。false代表该组数据由gp模型预测得到。 |
| fitness | 寻优算法优化值，该值越小代表该组参数效果越好 |
| num_prompts | 记录这次寻优测评工具发送的请求数。 |

其余列为对应的MindIE或VLLM的config.toml参数。

## 附录
### 配置文件说明

**寻优参数**： `n_particles` （寻优种子数）、`iters` （迭代轮次数）、 `tpot_slo` （`time_per_output_token`的限制时延）等。
用户可根据预估时间来自行配置种子和迭代次数。我们单个种子使用时间为拉起服务+测试数据。比如用户拉起服务+完成测试需9-10min，且愿意用8小时来进行寻优，则一共可跑约50个种子，建议用户配置5 * 10。设置种子数为10，迭代次数为5，建议用户配置种子数为迭代次数的2倍左右。
|参数|可选/必选|说明|
|---|---|---|
|n_particles|必选|寻优种子数，即一组生成的参数组合数，取值范围为：1-1000的整数。建议设为 15 ~ 30。 |
|iters|必选|迭代轮次数，取值范围为：1-1000的整数。建议设为 5 ~ 10。 |
|ttft_penalty|必选|`time_to_first_token` 即首token时延超时惩罚系数，若对 `time_to_first_token` 没有时延要求设置为0即可。取值范围：【0, 100】。建议设为1。| 
|tpot_penalty|必选|`time_per_output_token` 即非首token时延超时惩罚系数，若对`time_per_output_token`没有时延要求设置为0即可。取值范围：【0, 100】。建议设为1。|
|success_rate_penalty|必选|请求成功率惩罚系数，取值范围为：1-1000的整数。建议设为5。 |
|ttft_slo|必选|`time_to_first_token`的限制时延。如对`time_to_first_token`限制为2s内，则设为2，取值范围：(0, 100]，单位s。|
|tpot_slo|必选|`time_per_output_token`的限制时延。如对`time_per_output_token`限制为50ms内，则设为0.05，取值范围：(0, 100]，单位s。 |
|service|必选|标注多机启动时为主机或从机，多机场景下从机设为 `slave`，可取值：<br>&#8226;master：主机<br/>&#8226;slave：从机，<br/>默认值为`master`。|
|sample_size|可选|对原始数据集采样大小，用采样后的数据进行调优，可增加寻优效率，取值范围为：1000-10000的整数，建议设为原数据集请求的1 / 3。|

**测评工具参数**：
若使用`ais_bench`测评，需修改以下参数，可以参照[ais_bench 使用说明](https://gitee.com/aisbench/benchmark/blob/master/README.md)进行修改。

|参数|说明|
|---|---|
|models| 指定模型任务，可根据[模型配置说明](https://gitee.com/aisbench/benchmark/blob/master/doc/users_guide/models.md)进行配置。|
|datasets| 指定数据集任务，可根据[数据集准备指南](https://gitee.com/aisbench/benchmark/blob/master/doc/users_guide/datasets.md)进行配置 |
|mode| 运行模式。可根据[运行模式说明](https://gitee.com/aisbench/benchmark/blob/master/doc/users_guide/mode.md)进行配置。| 
|num_prompts| 控制运行数据集的条数，`mode`为`perf`时有效。| 

若使用`vllm_benchmark`测评，需修改以下参数：

|参数|可选/必选|说明|
|---|---|---|
|host|必选| 主机ip，需与`[vllm.command]`中的`host`保持一致，可设为`127.0.0.1`。|
|port|必选| 端口号，需与`[vllm.command]`中的`port`保持一致。| 
|model|必选| 模型路径，需与`[vllm.command]`中的`model`保持一致。| 
|served_model_name|必选| 模型名称，需与`[vllm.command]`中的`served_model_name`保持一致。|
|dataset_name|必选| 数据集名称。|
|dataset_path|必选| 数据集路径。|
|num_prompts|必选| 控制运行数据集的条数。| 取值范围：1-10000的整数|
|others|可选| 拼接其他参数，注意参数间使用空格分隔，参数内部不能留有空格。如`--ignore-eos --custom-output-len 1500`。默认为空。| 

**服务化参数**： 可以参考[MindIE server 配置参数说明](https://www.hiascend.com/document/detail/zh/mindie/20RC1/mindieservice/servicedev/mindie_service0285.html)进行修改。
服务化参数可直接指定参数的范围，如配置服务化参数 `max_batch_size` 的寻优搜索空间为 10 ~ 400，则可设置：
```
[[mindie.target_field]]
"name": "max_batch_size",    # 服务化参数名称
"config_position": "BackendConfig.ScheduleConfig.maxBatchSize",    # 服务化参数在MindIE Server中的位置
"min": 10,    # 最小值
"max": 400,    # 最大值
"dtype": "int"    # 数据类型
```
此外，也可设置参数与另一参数相关，如 `max_prefill_batch_size` 与 `max_batch_size` 相关，`max_prefill_batch_size = ratio * max_batch_size (0 < ratio < 1)`则可设置：
```
[[mindie.target_field]]
"name": "max_prefill_batch_size",
"config_position": "BackendConfig.ScheduleConfig.maxPrefillBatchSize",
"min": 0,
"max": 1,
"dtype": "ratio", 
"dtype_param": "max_batch_size"    # 表明该参数与max_batch_size相关
```
使用vllm框架时，需修改`config.toml`中的`[vllm.command]`参数，如：
```
[vllm.command]
host = "127.0.0.1"
port = 8000
model = "/workspace/vllm/models/llama-2-7b-chat-hf"
served_model_name = "llama-2-7b-chat-hf"
others = ""
```
|参数|可选/必选|说明|
|---|---|---|
|host|必选| 主机ip，需与`[vllm_benchmark.command]`中的`host`保持一致，可设为`127.0.0.1`。|
|port|必选| 端口号，需与`[vllm_benchmark.command]`中的`port`保持一致。| 
|model|必选| 模型路径，需与`[vllm_benchmark.command]`中的`model`保持一致。| 
|served_model_name|必选| 模型名称，需与`[vllm_benchmark.command]`中的`served_model_name`保持一致。|
|others|可选| 拼接其他参数，注意参数间使用空格分隔，参数内部不能留有空格。如：`--tensor-parallel-size 2 --no-enable-prefix-caching`。默认为空。| 

### PD分离寻优
服务化自动寻优工具支持在MindIE的A2单机PD分离场景中进行参数寻优（仅支持轻量化模式），且需要k8s部署。需保证能正常使用k8s拉起MindIE服务。
需要在`config.toml`中配置kubectl_default_path字段，将该字段配置为k8s安装脚本解压后的单机执行目录，目录结构需要为：
```
K8s_v1.23_MindCluster.7.1.RC1.B098.aarch/
├── all_label_a2.sh
├── all_label_a3.sh
├── Ascend-docker-runtime_7.1.RC1_linux-aarch64.run
├── Ascend-mindxdl-ascend-operator_7.1.RC1_linux-aarch64/
├── Ascend-mindxdl-clusterd_7.1.RC1_linux-aarch64/
├── Ascend-mindxdl-device-plugin_7.1.RC1_linux-aarch64/
├── Ascend-mindxdl-noded_7.1.RC1_linux-aarch64/
├── Ascend-mindxdl-volcano_7.1.RC1_linux-aarch64/
├── k8s
│   ├── alpine.tar
│   ├── calico3_23.yaml
│   ├── k8s1_23_0+calico3_23.tar.gz
│   └── ubuntu-18.04.tar
└── kubernetes
│   ├── Packages.gz
│   ├── kubeadm_1.23.0-00_arm64.deb
│   ├── kubectl_1.23.0-00_arm64.deb
│   ├── kubelet_1.23.0-00_arm64.deb
│   ├── ...
│   └── zlib1g_1%3a1.2.11.dfsg-2ubuntu9.2_arm64.deb
└── kubernetes_deploy_scripts_latest
    ├──boot_helper
    ├──chat.sh
    ├──conf
    ├──delete.sh
    ├──deploy_ac_job.py
    ├──deployment
    ├──deploy.sh
    ├──envcheck.sh
    ├──gen_ranktable_helper
    ├──log.sh
    ├──pd_scripts_single
    ├──show_logs.sh
    ├──user_config.json
    ├──user_config_base_A3.json
```
即配置
```
kubectl_default_path = "K8s_v1.23_MindCluster.7.1.RC1.B098.aarch/kubernetes_deploy_scripts_latest" #使用绝对路径
```
如果需要配置pd配比的参数寻优只需在config.toml的mindie配置中添加如下参数：
```
[[mindie.target_field]]
name = "default_p_rate"
config_position = "default_p_rate"
min = 1
max = 3
dtype = "int"
value = 1
[[mindie.target_field]]
name = "default_d_rate"
config_position = "default_d_rate"
min = 1
max = 3
dtype = "share"    # 表明该参数与default_p_rate相关，两者之和为定值
dtype_param = "default_p_rate"
```
### [插件模式](../msserviceprofiler/modelevalstate/optimizer/plugins/plugin.md)
现在寻优工具支持用户自定义推理框架以及测试工具，用户可以根据自己的需求配置。只需适配我们的插件模式，注册对应的插件即可。

### 日志说明
寻优过程中默认日志为INFO级别，如果用户想看每一轮具体的日志，可以在使用工具前设置
```
export MODELEVALSTATE_LEVEL=DEBUG
```
对于每一轮的运行状态会进行输出，我们将具体的MindIE/VLLM日志重定向在/tmp目录下，可以根据打屏信息获取具体文件路径查看MindIE/VLLM运行状态。