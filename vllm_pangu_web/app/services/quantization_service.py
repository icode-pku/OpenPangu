#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–æœåŠ¡æ¨¡å—
"""

import os
import json
import time
import torch
import transformers
from typing import Dict, Any
# from transformers import BitsAndBytesConfig
# from peft import prepare_model_for_kbit_training


# from datasets import load_dataset
# from llmcompressor import oneshot
# from llmcompressor.modifiers.quantization import GPTQModifier
# from llmcompressor.modifiers.smoothquant import SmoothQuantModifier
# from llmcompressor.utils import dispatch_for_generation

# from transformers import AutoTokenizer
# from .modeling_openpangu_dense import PanguEmbeddedForCausalLM

#apiç¤ºä¾‹ 
def quantize_model_with_w8a8(
    model_path: str,
    output_path: str,
    quantization_bits: int = 8,
    max_sequence_length: int = 4096,
    num_calibration_samples: int = 512,
) -> Dict[str, Any]:
    """ä½¿ç”¨W8A8å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–  å½“å‰PanguEmbeddedForCausalLMç‰ˆæœ¬ä¸æ”¯æŒé›†æˆåˆ°vLLMç³»ç»Ÿä¸­ï¼Œä»£ç ä»…ä½œæ¼”ç¤ºç”¨é€”ï¼Œå…·ä½“ç›˜å¤æ¨¡å‹çš„é‡åŒ–åŠŸèƒ½è¯·å‚è€ƒvllm_pangu_web/quantizationç›®å½•readmeæ–‡æ¡£"""
    # try:
    #     print(f"ğŸš€ å¼€å§‹é‡åŒ–æ¨¡å‹: {model_path} -> {output_path}")
    #     print(f"ğŸ”¢ é‡åŒ–ä½æ•°: {quantization_bits}bit")
        
    #     if not os.path.exists(model_path):
    #         return {"status": "error", "message": f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"}
        
    #     start_time = time.time()


    #     model = PanguEmbeddedForCausalLM.from_pretrained(model_path, dtype="auto", trust_remote_code=True)
    #     tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    #     # Select calibration dataset.
    #     DATASET_ID = "HuggingFaceH4/ultrachat_200k"
    #     DATASET_SPLIT = "train_sft"

    #     # Select number of samples. 512 samples is a good place to start.
    #     # Increasing the number of samples can improve accuracy.

    #     # Load dataset and preprocess.
    #     ds = load_dataset(DATASET_ID, split=f"{DATASET_SPLIT}[:{num_calibration_samples}]")
    #     ds = ds.shuffle(seed=42)


    #     def preprocess(example):
    #         return {
    #             "text": tokenizer.apply_chat_template(
    #                 example["messages"],
    #                 tokenize=False,
    #             )
    #         }


    #     ds = ds.map(preprocess)


    #     # Tokenize inputs.
    #     def tokenize(sample):
    #         return tokenizer(
    #             sample["text"],
    #             padding=False,
    #             max_length=max_sequence_length,
    #             truncation=True,
    #             add_special_tokens=False,
    #         )


    #     ds = ds.map(tokenize, remove_columns=ds.column_names)

    #     # Configure algorithms. In this case, we:
    #     #   * apply SmoothQuant to make the activations easier to quantize
    #     #   * quantize the weights to int8 with GPTQ (static per channel)
    #     #   * quantize the activations to int8 (dynamic per token)
    #     recipe = [
    #         SmoothQuantModifier(smoothing_strength=0.8),
    #         GPTQModifier(targets="Linear", scheme="W8A8", ignore=["lm_head"]),
    #     ]

    #     # Apply algorithms and save to output_dir
    #     oneshot(
    #         model=model,
    #         dataset=ds,
    #         recipe=recipe,
    #         max_seq_length=max_sequence_length,
    #         num_calibration_samples=num_calibration_samples,
    #         trust_remote_code_model=True,
    #     )

    #     # Confirm generations of the quantized model look sane.
    #     print("\n\n")
    #     print("========== SAMPLE GENERATION ==============")
    #     dispatch_for_generation(model)
    #     input_ids = tokenizer("Hello my name is", return_tensors="pt").input_ids.to("npu")
    #     output = model.generate(input_ids, max_new_tokens=100)
    #     print(tokenizer.decode(output[0]))
    #     print("==========================================\n\n")

    #     # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    #     os.makedirs(output_path, exist_ok=True)

    #     print("ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
    #     model.save_pretrained(output_path, save_compressed=True)
    #     tokenizer.save_pretrained(output_path)
        
    #     quantization_time = time.time() - start_time
        
    #     print(f"âœ… æ¨¡å‹é‡åŒ–æˆåŠŸ: {output_path}")
    #     print(f"â±ï¸  é‡åŒ–æ—¶é—´: {quantization_time:.2f}ç§’")
        
    #     return {
    #         "status": "success",
    #         "message": f"æ¨¡å‹é‡åŒ–æˆåŠŸ ({quantization_bits}bit)",
    #         "quantization_time": quantization_time,
    #         "output_path": output_path,
    #         "quantization_bits": quantization_bits
    #     }
        
    # except Exception as e:
    #     print(f"âŒ æ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
    #     return {"status": "error", "message": f"æ¨¡å‹é‡åŒ–å¤±è´¥: {str(e)}"}