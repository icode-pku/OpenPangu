#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ç®¡ç†æœåŠ¡
"""

import time
from typing import Dict, Optional, Any
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.usage.usage_lib import UsageContext

from app.models.state import global_state
from config import MODEL_PATH, TENSOR_PARALLEL_SIZE

async def load_vllm_model(config: Dict) -> Dict[str, Any]:
    """åŠ è½½vLLMæ¨¡å‹"""
    try:
        model_path = config.get("model_path", MODEL_PATH)
        quantization = config.get("quantization", "none")
        
        print(f"ğŸš€ å¼€å§‹åŠ è½½vLLMæ¨¡å‹: {model_path}")
        print(f"ğŸ”¢ é‡åŒ–æ–¹æ³•: {quantization}")
    
        # ä½¿ç”¨é…ç½®çš„å¼ é‡å¹¶è¡Œå¤§å°
        tensor_parallel_size = config.get("tensor_parallel_size", TENSOR_PARALLEL_SIZE)
        
        # æ„å»ºå¼•æ“å‚æ•°
        engine_args_dict = {
            "model": model_path,
            "tensor_parallel_size": tensor_parallel_size,
            "trust_remote_code": True,
            "max_num_seqs": config.get("max_num_seqs", 32),
            "max_model_len": config.get("max_model_len", 16384),
            "max_num_batched_tokens": config.get("max_num_batched_tokens", 4096),
            "tokenizer_mode": "slow",
            "gpu_memory_utilization": config.get("gpu_memory_utilization", 0.9),
            "dtype": "auto",
        }
        
        print(f"ğŸ”§ ä½¿ç”¨å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
        print(f"ğŸ”§ é‡åŒ–é…ç½®: {quantization}")
        
        engine_args = AsyncEngineArgs(**engine_args_dict)
        
        # åŠ è½½å¼•æ“
        start_time = time.time()
        engine = AsyncLLMEngine.from_engine_args(
            engine_args,
            usage_context=UsageContext.API_SERVER
        )
        load_time = time.time() - start_time
        
        # æ›´æ–°å…¨å±€çŠ¶æ€
        global_state.engine = engine
        global_state.current_model_config = config.copy()
        global_state.model_status.update({
            "loaded": True,
            "loading": False,
            "model_name": model_path.split("/")[-1],
            "model_type": "vllm",
            "quantization": quantization,
            "load_time": load_time,
            "memory_used": config.get("gpu_memory_utilization", 0.9) * 100,
            "tensor_parallel_size": tensor_parallel_size
        })
        
        print(f"âœ… vLLMæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"â±ï¸  åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")

        # ===== å®‰å…¨çš„æ¨¡å‹ä¿¡æ¯æ‰“å° =====
        print("\n" + "="*50)
        print("ğŸ“‹ å·²åŠ è½½çš„æ¨¡å‹ç»“æ„ä¿¡æ¯:")
        print("="*50)
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = engine.model_config

        if hasattr(model_config, 'model'):
            print(f"æ¨¡å‹è·¯å¾„: {model_config.model}")
        
        if hasattr(model_config, 'architecture'):
            print(f"æ¨¡å‹æ¶æ„: {model_config.architecture}")
        
        if hasattr(model_config, 'hidden_size'):
            print(f"éšè—å±‚ç»´åº¦: {model_config.hidden_size}")
        
        if hasattr(model_config, 'hf_config'):
            print(f"HuggingFaceé…ç½®å¯¹è±¡: {model_config.hf_config}")
        
        if hasattr(model_config, 'dtype'):
            print(f"æ•°æ®ç±»å‹: {model_config.dtype}")
        
        return {
            "status": "success", 
            "message": "vLLMæ¨¡å‹åŠ è½½æˆåŠŸ",
            "load_time": load_time,
            "model_name": global_state.model_status["model_name"],
            "device_type": "gpu",
            "tensor_parallel_size": tensor_parallel_size,
            "quantization": quantization
        }
        
    except Exception as e:
        global_state.model_status["loading"] = False
        global_state.model_status["loaded"] = False
        print(f"âŒ vLLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return {"status": "error", "message": f"vLLMæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"}

async def load_model_with_config(config: Dict) -> Dict[str, Any]:
    """æ ¹æ®é…ç½®åŠ è½½æ¨¡å‹"""
    if global_state.model_status["loading"]:
        return {"status": "error", "message": "æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­"}
    
    if global_state.model_status["loaded"]:
        return {"status": "error", "message": "æ¨¡å‹å·²åŠ è½½ï¼Œè¯·å…ˆå¸è½½"}
    
    global_state.model_status["loading"] = True
    
    model_type = config.get("model_type", "vllm")
    
    if model_type == "vllm" and global_state.vllm_available:
        return await load_vllm_model(config)
    else:
        if global_state.vllm_available:
            return await load_vllm_model(config)
        else:
            global_state.model_status["loading"] = False
            return {"status": "error", "message": "æ— å¯ç”¨æ¨ç†åç«¯"}

async def unload_model() -> Dict[str, Any]:
    """å¸è½½æ¨¡å‹"""
    if not global_state.model_status["loaded"]:
        return {"status": "warning", "message": "æ²¡æœ‰åŠ è½½çš„æ¨¡å‹"}
    
    try:
        print("ğŸ”„ å¼€å§‹å¸è½½æ¨¡å‹...")
        
        if global_state.model_status["model_type"] == "vllm" and global_state.engine is not None:
            # æ¸…ç†vLLMå¼•æ“
            try:
                global_state.engine.shutdown()
            except:
                pass
            global_state.engine = None
            print("âœ… vLLMæ¨¡å‹å¸è½½æˆåŠŸ")
        
        # é‡ç½®çŠ¶æ€
        global_state.model_status.update({
            "loaded": False,
            "loading": False,
            "model_name": "",
            "model_type": "",
            "quantization": "none",
            "load_time": None,
            "memory_used": 0,
            "tensor_parallel_size": 1
        })
        global_state.current_model_config = {}
        
        # æ¸…ç†å¯¹è¯å†å²
        global_state.clear_conversations()
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰å¯¹è¯å†å²")
        
        return {"status": "success", "message": "æ¨¡å‹å¸è½½æˆåŠŸ"}
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹å¸è½½å¤±è´¥: {e}")
        return {"status": "error", "message": f"æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"}